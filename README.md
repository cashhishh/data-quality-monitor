# Data Quality Monitoring System

A full-stack data quality monitoring platform that allows users to upload datasets, run automated data quality checks, store results in a database, and visualize insights using Power BI.

This project is built to simulate a real-world data engineering + analytics workflow.

---

## ğŸš€ Features

- Upload datasets (CSV)
- Store dataset metadata in SQL Server
- Run automated data quality checks:
  - Null value detection
  - Duplicate row detection
- Persist validation results in the database
- REST APIs built with FastAPI
- Frontend dashboard for dataset management
- Power BI integration for analytics and visualization
- Synthetic large dataset generation for realistic analysis

---

## ğŸ—ï¸ Tech Stack

### Backend
- Python
- FastAPI
- SQL Server (ODBC Driver 17)
- Pandas
- PyODBC

### Frontend
- React
- JavaScript
- Axios
- Basic UI components

### Analytics
- Power BI
- SQL Server as data source

---

## ğŸ“‚ Project Structure

data-quality-monitor/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ routes/
â”‚ â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â”œâ”€â”€ db.py
â”‚ â”‚ â””â”€â”€ main.py
â”‚ â””â”€â”€ scripts/
â”‚ â””â”€â”€ generate_synthetic_data.py
â”‚
â”œâ”€â”€ frontend/
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/cashhishh/data-quality-monitor.git
cd data-quality-monitor

2ï¸âƒ£ Backend setup
cd backend
python -m venv venv
source venv/Scripts/activate   # Windows
pip install -r requirements.txt


Update database connection in app/db.py:

def get_connection():
    return pyodbc.connect(
        "DRIVER={ODBC Driver 17 for SQL Server};"
        "SERVER=.\\SQLEXPRESS;"
        "DATABASE=DataQualityDB;"
        "Trusted_Connection=yes;"
    )


Run backend:

uvicorn app.main:app --reload


API Docs available at:

http://127.0.0.1:8000/docs

3ï¸âƒ£ Frontend setup
cd frontend
npm install
npm start

ğŸ“Š Power BI Integration

Connect Power BI to SQL Server

Use tables:

datasets

dataset_records

validation_results

Build visuals such as:

Dataset count

Null percentage

Duplicate rows

Upload trends over time

ğŸ§ª Synthetic Data

To generate large datasets for realistic analysis:

python backend/scripts/generate_synthetic_data.py


This helps simulate real-world data volumes instead of small demo files.

ğŸ”® Future Enhancements

Column-level data quality rules

Schema validation

Data freshness checks

Scheduled quality checks

Authentication & user roles

Cloud deployment (Docker + Azure/AWS)

ğŸ“Œ Why this project?

This project demonstrates:

Backend API design

Database integration

Data quality concepts

Analytics + visualization

End-to-end data pipeline thinking

ğŸ‘©â€ğŸ’» Author

Kashish
B.Tech ECE | Data & Software Enthusiast

